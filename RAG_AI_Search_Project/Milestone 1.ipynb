{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YzICc7GNDQS"
      },
      "source": [
        "# RAG Mini Project\n",
        "## Milestone #1 : Create and store Chunks\n",
        "This notebook shows how to create text chunks from MS Word Documents.  \n",
        "I used five Word Documents on the topic of Agentic AI. \n",
        "\n",
        "- Chunks all the word documents in a directory\n",
        "- Uses python-docs to extract paragraph text for chunking\n",
        "- Paragraphs are merged depending on parameterizable  max chunk size\n",
        "- Document cleaning recommended for best results\n",
        "- remove diagrams and unnecessary text\n",
        "- merge paragraphs that are semantically similar\n",
        "\n",
        "## Deliverables:\n",
        "- Selection of multiple documents for your RAG project\n",
        "- Capture chunks in a pickle file for next step (Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting lxml>=3.1.0\n",
            "  Downloading lxml-5.3.1-cp311-cp311-macosx_10_9_universal2.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.9.0 in /Users/aigerim/Library/Python/3.11/lib/python/site-packages (from python-docx) (4.12.2)\n",
            "Installing collected packages: lxml, python-docx\n",
            "Successfully installed lxml-5.3.1 python-docx-1.1.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Agentic AI refers to artificial intelligence systems that exhibit autonomous decision-making, adaptability, and goal-directed behavior. Unlike traditional AI, which primarily follows predefined rules or relies on statistical pattern recognition, agentic AI is characterized by its ability to plan, reason, and take initiative in dynamic environments. This type of AI is particularly relevant for applications that require independent problem-solving, such as robotics, autonomous agents, and strategic decision-making systems.']\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "\n",
        "def extract_text(file_path):\n",
        "    \"\"\"Extracts and cleans text from a .docx file.\"\"\"\n",
        "    doc = Document(file_path)\n",
        "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "# Example usage\n",
        "file_path = \"Agentic_AI_Introduction.docx\"  # Replace with your actual file path\n",
        "paragraphs = extract_text(file_path)\n",
        "print(paragraphs[:5])  # Print first 5 paragraphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.11.0\n",
            "  Downloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.20.0\n",
            "  Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting Pillow\n",
            "  Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.11.0)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/aigerim/Library/Python/3.11/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aigerim/Library/Python/3.11/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Collecting networkx\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Collecting sympy==1.13.1\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.17\n",
            "  Downloading numpy-2.2.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.4.1\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2022.12.7)\n",
            "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, Pillow, numpy, networkx, MarkupSafe, joblib, fsspec, scipy, jinja2, huggingface-hub, torch, tokenizers, scikit-learn, transformers, sentence-transformers\n",
            "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 fsspec-2025.2.0 huggingface-hub-0.29.1 jinja2-3.1.5 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-3.4.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.49.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed-size chunks: ['Agentic AI refers to artificial intelligence systems that exhibit autonomous decision-making, adaptability, and goal-directed behavior. Unlike traditional AI, which primarily follows predefined rules or relies on statistical pattern recognition, agentic AI is characterized by its ability to plan, reason, and take initiative in dynamic environments. This type of AI is particularly relevant for applications that require independent problem-solving, such as robotics, autonomous agents, and strategic decision-making systems.']\n",
            "Semantic chunks: ['Agentic AI refers to artificial intelligence systems that exhibit autonomous decision-making, adaptability, and goal-directed behavior. Unlike traditional AI, which primarily follows predefined rules or relies on statistical pattern recognition, agentic AI is characterized by its ability to plan, reason, and take initiative in dynamic environments. This type of AI is particularly relevant for applications that require independent problem-solving, such as robotics, autonomous agents, and strategic decision-making systems.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load pre-trained model for semantic similarity\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def chunk_text_fixed(paragraphs, chunk_size=500):\n",
        "    \"\"\"Chunks text into fixed-size pieces.\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if len(current_chunk) + len(paragraph) > chunk_size:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = paragraph\n",
        "        else:\n",
        "            current_chunk += \" \" + paragraph if current_chunk else paragraph\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def chunk_text_semantic(paragraphs, similarity_threshold=0.8):\n",
        "    \"\"\"Merges semantically similar paragraphs.\"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = paragraphs[0]\n",
        "\n",
        "    for i in range(1, len(paragraphs)):\n",
        "        sim_score = util.pytorch_cos_sim(model.encode(current_chunk), model.encode(paragraphs[i])).item()\n",
        "\n",
        "        if sim_score >= similarity_threshold:\n",
        "            current_chunk += \" \" + paragraphs[i]\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = paragraphs[i]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Test with one document\n",
        "paragraphs = extract_text(\"Agentic_AI_Introduction.docx\")  # Replace with your actual file\n",
        "fixed_chunks = chunk_text_fixed(paragraphs, chunk_size=500)\n",
        "semantic_chunks = chunk_text_semantic(paragraphs, similarity_threshold=0.8)\n",
        "\n",
        "print(\"Fixed-size chunks:\", fixed_chunks[:3])  # Print first 3 chunks\n",
        "print(\"Semantic chunks:\", semantic_chunks[:3])  # Print first 3 chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Agentic_AI_Applications.docx\n",
            "Processing: Agentic_AI_Technical_Aspects.docx\n",
            "Processing: Agentic_AI_Future_Trends.docx\n",
            "Processing: Agentic_AI_Challenges.docx\n",
            "Processing: Agentic_AI_Introduction.docx\n",
            "Total Fixed-size Chunks: 5\n",
            "Total Semantic Chunks: 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List all .docx files in the working directory\n",
        "docx_files = [f for f in os.listdir() if f.endswith('.docx')]\n",
        "\n",
        "all_chunks_fixed = []\n",
        "all_chunks_semantic = []\n",
        "\n",
        "for file in docx_files:\n",
        "    print(f\"Processing: {file}\")\n",
        "    paragraphs = extract_text(file)\n",
        "\n",
        "    # Fixed-size chunking\n",
        "    fixed_chunks = chunk_text_fixed(paragraphs, chunk_size=500)\n",
        "    all_chunks_fixed.extend(fixed_chunks)\n",
        "\n",
        "    # Semantic chunking\n",
        "    semantic_chunks = chunk_text_semantic(paragraphs, similarity_threshold=0.8)\n",
        "    all_chunks_semantic.extend(semantic_chunks)\n",
        "\n",
        "print(f\"Total Fixed-size Chunks: {len(all_chunks_fixed)}\")\n",
        "print(f\"Total Semantic Chunks: {len(all_chunks_semantic)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks saved successfully to chunks.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save chunks into a pickle file\n",
        "with open(\"chunks.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\"fixed_chunks\": all_chunks_fixed, \"semantic_chunks\": all_chunks_semantic}, f)\n",
        "\n",
        "print(\"Chunks saved successfully to chunks.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pickle file successfully!\n",
            "Fixed chunks sample: ['Agentic AI has diverse real-world applications across various industries. In healthcare, it can optimize treatment plans based on patient data. In finance, it enhances algorithmic trading by making independent market predictions. In cybersecurity, agentic AI can autonomously detect and mitigate threats in real time. Furthermore, its integration into robotics enables self-driving cars and industrial automation systems to function with minimal human intervention.', 'The technical foundation of Agentic AI includes reinforcement learning, neuro-symbolic AI, and multi-agent systems. Reinforcement learning enables AI to optimize decision-making through trial and error, while neuro-symbolic approaches integrate logical reasoning with neural networks to enhance adaptability. Multi-agent systems, where multiple AI entities collaborate or compete, further enhance the robustness and scalability of agentic AI. These elements collectively allow AI to exhibit autonomous problem-solving capabilities.']\n",
            "Semantic chunks sample: ['Agentic AI has diverse real-world applications across various industries. In healthcare, it can optimize treatment plans based on patient data. In finance, it enhances algorithmic trading by making independent market predictions. In cybersecurity, agentic AI can autonomously detect and mitigate threats in real time. Furthermore, its integration into robotics enables self-driving cars and industrial automation systems to function with minimal human intervention.', 'The technical foundation of Agentic AI includes reinforcement learning, neuro-symbolic AI, and multi-agent systems. Reinforcement learning enables AI to optimize decision-making through trial and error, while neuro-symbolic approaches integrate logical reasoning with neural networks to enhance adaptability. Multi-agent systems, where multiple AI entities collaborate or compete, further enhance the robustness and scalability of agentic AI. These elements collectively allow AI to exhibit autonomous problem-solving capabilities.']\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"chunks.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "print(\"Loaded pickle file successfully!\")\n",
        "print(\"Fixed chunks sample:\", data[\"fixed_chunks\"][:2])\n",
        "print(\"Semantic chunks sample:\", data[\"semantic_chunks\"][:2])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
